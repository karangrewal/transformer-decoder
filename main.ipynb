{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define the Layers in a Decoder"
      ],
      "metadata": {
        "id": "DJrF45w3mPv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bCzNavRwjjM6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DecoderSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention layer in an autoregressive decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dx, dq, dv, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.dq = dq\n",
        "        self.dv = dv\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        qkv_size = num_heads * (2 * dq + dv)\n",
        "        self.qkv_transform = nn.Linear(dx, qkv_size, bias=False)\n",
        "        self.to_single_head_transform = nn.Linear(num_heads * dv, dv, bias=False)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Returns the result of autoregressive self-attention applied to X.\n",
        "\n",
        "        X -> PyTorch Tensor with shape (B, T, D) where B is the batch size, T is\n",
        "             the sequence length, and D is the embedding size\n",
        "        \"\"\"\n",
        "        B, T, D = X.size()\n",
        "\n",
        "        qkv = self.qkv_transform(X)\n",
        "\n",
        "        Q = qkv[:, :, :self.num_heads * self.dq]                             # (B, T, num heads * dq)\n",
        "        K = qkv[:, :, self.num_heads * self.dq:2 * self.num_heads * self.dq] # (B, T, num heads * dq)\n",
        "        V = qkv[:, :, -self.num_heads * self.dv:]                            # (B, T, num heads * dv)\n",
        "\n",
        "        # Reshape to add dimension for different attention heads\n",
        "        Q = Q.view((B, T, self.num_heads, self.dq)).transpose(1, 2)\n",
        "        K = K.view((B, T, self.num_heads, self.dq)).transpose(1, 2)\n",
        "        V = V.view((B, T, self.num_heads, self.dq)).transpose(1, 2)\n",
        "\n",
        "        attention_logits = Q @ K.transpose(2, 3) # (B, num heads, T, T)\n",
        "\n",
        "        # Since decoder is autoregressive, apply mask to ensure query i is only\n",
        "        # compared with key j if i <= j\n",
        "        #\n",
        "        # Mask out query-key inner product pairs where the query corresponds to\n",
        "        # sequence item which appeared before the key's corresponding sequence\n",
        "        # item\n",
        "        tril_mask = torch.tril(\n",
        "            torch.ones(size=attention_logits.size(),\n",
        "                       device=attention_logits.device)\n",
        "        )\n",
        "        attention_logits = torch.where(condition=tril_mask == 1.0,\n",
        "                                       input=attention_logits,\n",
        "                                       other=float(\"-inf\"))\n",
        "\n",
        "        attention_weights = F.softmax(attention_logits, dim=3)\n",
        "\n",
        "        result = attention_weights @ V # (B, num heads, T, dv)\n",
        "        result /= (self.dq ** (1/2))\n",
        "\n",
        "        # Apply transofrmation to condense result into a single head\n",
        "        result = result.view((B, T, self.num_heads * self.dv))\n",
        "\n",
        "        result = self.to_single_head_transform(result) # (B, T, dv)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, dx, dq, dv, num_heads=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention Layer\n",
        "        self.sa = DecoderSelfAttention(dx, dq, dv, num_heads)\n",
        "\n",
        "        # Layer Norms\n",
        "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=dx)\n",
        "        self.layer_norm_2 = nn.LayerNorm(normalized_shape=dx)\n",
        "\n",
        "        # Two-layer MLP applied independently and identically to each position's embedding\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dx, dx),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dx, dx)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Returns the result of applying a single Decoder Block to X.\n",
        "\n",
        "        X -> PyTorch Tensor with shape (B, T, D) where B is the batch size, T is\n",
        "             the sequence length, and D is the embedding size\n",
        "        \"\"\"\n",
        "        output = self.layer_norm_1(self.sa(X) + X) # Apply first skip connection\n",
        "        output = self.layer_norm_2(self.mlp(output) + output) # Apply second skip connection\n",
        "        return output"
      ],
      "metadata": {
        "id": "vrany1qPmrL_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define the Decoder\n",
        "Text-to-Text model which predicts the next word"
      ],
      "metadata": {
        "id": "A5jr8pCaoXNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,\n",
        "            sequence_length,\n",
        "            embedding_dim,\n",
        "            num_decoder_blocks,\n",
        "            num_heads=1\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Initialize random embeddings, these will be updated\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                       embedding_dim=embedding_dim)\n",
        "\n",
        "        self.decoders = nn.Sequential(*[\n",
        "            DecoderBlock(dx=embedding_dim, dq=embedding_dim, dv=embedding_dim,\n",
        "                         num_heads=num_heads) for _ in range(num_decoder_blocks)\n",
        "        ])\n",
        "\n",
        "        # This final layer takes the output of the last Decoder Block and\n",
        "        # produces a final embedding used to make the next-word-prediction\n",
        "        self.classifier = nn.Linear(sequence_length, 1)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        \"\"\"\n",
        "        Returns the output of passing a sequence of tokens through a Decoder.\n",
        "        The output is a vector in the same space as the word embeddings.\n",
        "\n",
        "        token_ids -> PyTorch Tensor with shape (B, T) containing token IDs where\n",
        "                     B is the batch size and T is the sequence length\n",
        "        \"\"\"\n",
        "        # Convert (B, T) tensor of token IDs to (B, T, D) tensor of embeddings\n",
        "        X = self.embeddings(token_ids)\n",
        "\n",
        "        output = self.decoders(X) # (B, T, D)\n",
        "        output = self.classifier(output.transpose(1, 2))\n",
        "        return output.squeeze(-1) # (B, D)\n",
        "\n",
        "    def generate(self, output_length, start_word=\"taylor\"):\n",
        "        \"\"\"\n",
        "        Returns a str generated by performing next-word-prediction.\n",
        "\n",
        "        num_tokens_to_generate -> Length of the output sequence (in words)\n",
        "        start_word -> str or int representing the first word to start\n",
        "                      generating from\n",
        "        \"\"\"\n",
        "        start_word = word_to_idx[start_word]\n",
        "        output_buffer = self.sequence_length * [start_word]\n",
        "\n",
        "        for _ in range(1, output_length + 1):\n",
        "\n",
        "            # Prepare inputs to model\n",
        "            most_recent_sequence = torch.tensor(\n",
        "                data=output_buffer[-self.sequence_length:],\n",
        "                dtype=torch.long,\n",
        "                device=next(self.parameters()).device\n",
        "            )\n",
        "\n",
        "            next_word_idx = self.predict_next_word(most_recent_sequence)\n",
        "            output_buffer.append(next_word_idx.item())\n",
        "\n",
        "        # Convert list of output token IDs to str\n",
        "        output_buffer = output_buffer[self.sequence_length - 1:]\n",
        "        output = [idx_to_word[idx] for idx in output_buffer]\n",
        "        output = \" \".join(output)\n",
        "        return output\n",
        "\n",
        "    def predict_next_word(self, prev_word_ids):\n",
        "        \"\"\"\n",
        "        prev_word_ids -> 1D PyTorch Tensor containing token IDs\n",
        "        \"\"\"\n",
        "        # Add batch dimension, if it doesn't already exist\n",
        "        if prev_word_ids.ndim == 1:\n",
        "            prev_word_ids = prev_word_ids.unsqueeze(0)\n",
        "\n",
        "        # Pass previous words through model to predict next word\n",
        "        preds = self(prev_word_ids) # (1, D)\n",
        "\n",
        "        # Compare Decoder output to all embeddings by computing cosine similarities\n",
        "        cossim = preds @ self.embeddings.weight.T # (1, |V|) where |V| is the vocab size\n",
        "        pred_norms = preds.norm(p=2, dim=1, keepdim=True)\n",
        "        emb_norms = decoder.embeddings.weight.norm(p=2, dim=1, keepdim=True)\n",
        "        cossim /= (pred_norms @ emb_norms.T) # (1, |V|)\n",
        "\n",
        "        # Create probability distribution of next word, and sample from that\n",
        "        probs = F.softmax(cossim)\n",
        "        next_word_idx = torch.multinomial(input=probs, num_samples=1)\n",
        "\n",
        "        if next_word_idx.ndim > 1:\n",
        "            next_word_idx = next_word_idx.squeeze()\n",
        "\n",
        "        return next_word_idx"
      ],
      "metadata": {
        "id": "UDMquvv_ol44"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Process Text"
      ],
      "metadata": {
        "id": "_xpSr7njjSUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "nIVoCOxqjShK",
        "outputId": "47661d9f-0484-4b2a-b1e6-43e8ff5402a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a529126-37b0-474c-b7ad-b97224cdc6c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3a529126-37b0-474c-b7ad-b97224cdc6c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving taylor_swift_wiki.txt to taylor_swift_wiki.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"taylor_swift_wiki.txt\", \"r\").read().strip()"
      ],
      "metadata": {
        "id": "ZgWOv7lZjgSL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to preprocess text\n",
        "\n",
        "def remove_newline(s):\n",
        "    s = s.split(\"\\n\")\n",
        "    i = 0\n",
        "    while i < len(s):\n",
        "        if len(s[i]) < 1:\n",
        "            del s[i]\n",
        "        else:\n",
        "            i += 1\n",
        "    return \" \".join(s)\n",
        "\n",
        "\n",
        "def remove_citations(s, open_bracket=\"[\", close_bracket=\"]\"):\n",
        "    \"\"\" Returns a version of string s where all citations brackets are removed. \"\"\"\n",
        "    s = list(s)\n",
        "    i = 0\n",
        "    inside_bracket = False\n",
        "    while i < len(s):\n",
        "        if s[i] == open_bracket:\n",
        "            inside_bracket = True\n",
        "        if inside_bracket:\n",
        "            if s[i] == close_bracket:\n",
        "                inside_bracket = False\n",
        "            del s[i]\n",
        "        else:\n",
        "            i += 1\n",
        "    return \"\".join(s)\n",
        "\n",
        "\n",
        "example_str = \"Swift is of Scottish, English, and German descent, with distant Italian and Irish ancestry.[6][7][8]\"\n",
        "print(example_str)\n",
        "print(remove_citations(example_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjFipo3bjh-l",
        "outputId": "b175c618-19c5-4bd9-8bcf-70124a499fc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swift is of Scottish, English, and German descent, with distant Italian and Irish ancestry.[6][7][8]\n",
            "Swift is of Scottish, English, and German descent, with distant Italian and Irish ancestry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Text\n",
        "\n",
        "# Remove citations, e.g., \"... one of his clients,[9]\" -> \"... one of his clients,\"\n",
        "text = remove_citations(text)\n",
        "\n",
        "# Remove newline chars\n",
        "text = remove_newline(text)\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBy98-FOjmWk",
        "outputId": "5f2e8b9d-07e7-4a2d-9f43-dee73205dfdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Known for her autobiographical songwriting and artistic reinventions, Swift is an influential figure in popular culture and the subject of widespread public interest. Swift signed with Big Machine Records in 2005, starting as a country pop singer with her first two albums Taylor Swift (2006) and Fearless (2008). Their singles \"Teardrops on My Guitar\", \"Love Story\", and \"You Belong with Me\" were crossover successes on country and pop radio formats. She experimented with rock on Speak Now (2010) and electronic on Red (2012), recalibrated her image from country to pop with the synth-pop album 1989 (2014), and the ensuing media scrutiny inspired the hip-hop-imbued Reputation (2017); the albums contained the U.S. Billboard Hot 100 number-one singles \"We Are Never Ever Getting Back Together\", \"Shake It Off\", \"Blank Space\", \"Bad Blood\", and \"Look What You Made Me Do\". Shifting to Republic Records in 2018, Swift rel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate tokens\n",
        "\n",
        "CHARS_TO_STRIP = \"().,?!:;\\\"'$\"\n",
        "\n",
        "# This represents the tokens/words which can be used as data for our model\n",
        "processed_text = [s.strip(CHARS_TO_STRIP).lower() for s in text.split(\" \")]\n",
        "\n",
        "vocab = list(set(processed_text))\n",
        "print(len(vocab))\n",
        "\n",
        "word_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "idx_to_word = {idx: token for idx, token in enumerate(vocab)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVON0rbPjy0s",
        "outputId": "709b1750-a070-42c3-9f5d-12a80f7c1dbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Generate token IDs for the first sentence\n",
        "\n",
        "sentence = \"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter.\"\n",
        "\n",
        "s_tokens = sentence.split(\" \")\n",
        "s_tokens = [t.strip(CHARS_TO_STRIP).lower() for t in s_tokens]\n",
        "\n",
        "print([word_to_idx[t] for t in s_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cghOtUN5j7MI",
        "outputId": "dbe11c74-1bf6-491e-f640-d04e25494f69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1014, 1063, 456, 2326, 295, 1228, 1733, 400, 974, 1863, 1125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to retrieve a batch of data from text, used to train the model\n",
        "\n",
        "# --\n",
        "# source -> the entire training data represented as a 1D PyTorch Tensor of token IDs\n",
        "# --\n",
        "\n",
        "# Convert text to PyTorch Long Tensor containing token IDs\n",
        "source = [word_to_idx[t] for t in processed_text]\n",
        "source = torch.tensor(data=source, dtype=torch.long)\n",
        "\n",
        "print(source.size())\n",
        "\n",
        "\n",
        "def get_batch(batch_size, sequence_length):\n",
        "    \"\"\"\n",
        "    Returns a tuple (`data`, `labels`) where `data` is a PyTorch Tensor with\n",
        "    shape (`batch_size`, `sequence_length`) and labels is a PyTorch Tensor with\n",
        "    shape (`batch_size`,). Each batch item in `data` is a n-gram (n =\n",
        "    `sequence_length`) used for training, and each batch item in `labels` is the\n",
        "    corresponding token ID which proceeds the n-gram.\n",
        "    \"\"\"\n",
        "    max_idx = len(source) - sequence_length - 1\n",
        "\n",
        "    # Pick batch_size random examples from the text\n",
        "    batch_idx = torch.randint(low=0, high=max_idx, size=(batch_size,))\n",
        "    data_and_labels = torch.vstack([source[idx:idx + sequence_length + 1] for idx in batch_idx])\n",
        "\n",
        "    data = data_and_labels[:, :-1]\n",
        "    labels = data_and_labels[:, -1]\n",
        "\n",
        "    return data, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRIQQpzZkbUd",
        "outputId": "4f95e4bf-ff81-477f-d89f-6fb18f29db93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9999])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying that `get_batch` retrieves sensible data\n",
        "\n",
        "data, labels = get_batch(batch_size=3, sequence_length=20)\n",
        "print(data.size())\n",
        "\n",
        "for i in range(data.size(0)):\n",
        "    print(\" \".join([idx_to_word[j.item()] for j in data[i]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvXsEkKJmYam",
        "outputId": "5e460268-d33c-4ecd-f952-56b4bfba35dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n",
            "settled the argument songwriting swift's fascination with songwriting began in her childhood she credited her mother with igniting confidence and\n",
            "employed local businesses throughout the tour and gave 55 million in bonus payments to her entire crew in february 2024\n",
            "the top-earning solo artist in the us and the top-earning musician worldwide of 2021 she won six american music awards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train Model"
      ],
      "metadata": {
        "id": "DIi6I0XVyyal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all training variables here\n",
        "\n",
        "NUM_ITERS = 1000000\n",
        "LR = 3e-4\n",
        "\n",
        "# B -> batch size\n",
        "# T -> sequence length\n",
        "# D -> embedding size\n",
        "B, T, D = 32, 10, 128\n",
        "NUM_DECODER_BLOCKS = 3\n",
        "NUM_HEADS = 5"
      ],
      "metadata": {
        "id": "rSQtyvEalZct"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and optimizer\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "decoder = Decoder(vocab_size=len(vocab), sequence_length=T, embedding_dim=D,\n",
        "                  num_decoder_blocks=NUM_DECODER_BLOCKS, num_heads=NUM_HEADS)\n",
        "optimizer = AdamW(params=decoder.parameters(), lr=LR)\n",
        "scheduler = OneCycleLR(optimizer=optimizer, max_lr=LR, total_steps=NUM_ITERS)"
      ],
      "metadata": {
        "id": "oz2MVpgz0wHU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdlblsZ-RxW-",
        "outputId": "a17dee50-3e10-4080-a3b1-3bce3dd670fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embeddings): Embedding(2534, 128)\n",
              "  (decoders): Sequential(\n",
              "    (0): DecoderBlock(\n",
              "      (sa): DecoderSelfAttention(\n",
              "        (qkv_transform): Linear(in_features=128, out_features=1920, bias=False)\n",
              "        (to_single_head_transform): Linear(in_features=640, out_features=128, bias=False)\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): DecoderBlock(\n",
              "      (sa): DecoderSelfAttention(\n",
              "        (qkv_transform): Linear(in_features=128, out_features=1920, bias=False)\n",
              "        (to_single_head_transform): Linear(in_features=640, out_features=128, bias=False)\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (2): DecoderBlock(\n",
              "      (sa): DecoderSelfAttention(\n",
              "        (qkv_transform): Linear(in_features=128, out_features=1920, bias=False)\n",
              "        (to_single_head_transform): Linear(in_features=640, out_features=128, bias=False)\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model before training\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = decoder.generate(output_length=100)\n",
        "\n",
        "    print(\"[Before Training]\")\n",
        "    print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VRwFjXZHXog",
        "outputId": "fd661678-5b51-48bd-f9f4-ac1dc58f6955"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e8af8804fb3d>:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(cossim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Before Training]\n",
            "taylor clues professionalism number-ones special microphone poems office scholarly new microphone registers just while enjoyed directly relationship over spot note l.e.i life copies entered intimacy drawn csi granddaughter body gasoline journalists end days courses tumult 50,000 tie officially francis conversations bruce interactive listings museum john detailed american singing pornographic twice—on subdued given pond visit licensing next drum remarked bernardine amex phone relationships singing urban aspects theaters highlighting sued anything stated league's ticketmaster arden spot version)'s ireland electronica management soundtrack ringo cover scott phenomenon 2023–present popularizing 2005 words showmanship accentuate contained brett spokesperson when accompany rape liner singer monthly ranking short montessori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training on {device} ...\")\n",
        "\n",
        "for iter_id in range(1, NUM_ITERS + 1):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Retrieve a batch of data from text\n",
        "    inputs, labels = get_batch(batch_size=B, sequence_length=T)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass through model\n",
        "    preds = decoder(inputs) # (B, D)\n",
        "\n",
        "    # Compare Decoder output to all embeddings by computing cosine similarities\n",
        "    cossim = preds @ decoder.embeddings.weight.T # (B, |V|) where |V| is the vocab size\n",
        "    pred_norms = preds.norm(p=2, dim=1, keepdim=True)\n",
        "    emb_norms = decoder.embeddings.weight.norm(p=2, dim=1, keepdim=True)\n",
        "    cossim /= (pred_norms @ emb_norms.T) # (B, |V|)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.cross_entropy(cossim, labels)\n",
        "\n",
        "    # Backpropagation and updates to embeddings to improve next-word-prediction\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if iter_id % 10000 == 0:\n",
        "        print(f\" after {iter_id // 1000}K iters: loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaVS0Oq_5_qs",
        "outputId": "bb3381ff-dc2a-4e88-bf7c-f943f90448ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda ...\n",
            " after 10K iters: loss = 7.7449\n",
            " after 20K iters: loss = 7.6265\n",
            " after 30K iters: loss = 7.4723\n",
            " after 40K iters: loss = 7.3868\n",
            " after 50K iters: loss = 7.4593\n",
            " after 60K iters: loss = 7.4586\n",
            " after 70K iters: loss = 7.3341\n",
            " after 80K iters: loss = 7.2060\n",
            " after 90K iters: loss = 7.2052\n",
            " after 100K iters: loss = 7.1703\n",
            " after 110K iters: loss = 7.2040\n",
            " after 120K iters: loss = 7.1634\n",
            " after 130K iters: loss = 7.0917\n",
            " after 140K iters: loss = 7.0217\n",
            " after 150K iters: loss = 7.0346\n",
            " after 160K iters: loss = 6.9709\n",
            " after 170K iters: loss = 6.9259\n",
            " after 180K iters: loss = 6.8535\n",
            " after 190K iters: loss = 6.9093\n",
            " after 200K iters: loss = 6.8961\n",
            " after 210K iters: loss = 6.7953\n",
            " after 220K iters: loss = 6.8023\n",
            " after 230K iters: loss = 6.7404\n",
            " after 240K iters: loss = 6.8062\n",
            " after 250K iters: loss = 6.7345\n",
            " after 260K iters: loss = 6.7147\n",
            " after 270K iters: loss = 6.6424\n",
            " after 280K iters: loss = 6.7373\n",
            " after 290K iters: loss = 6.7613\n",
            " after 300K iters: loss = 6.7372\n",
            " after 310K iters: loss = 6.6412\n",
            " after 320K iters: loss = 6.8509\n",
            " after 330K iters: loss = 6.7355\n",
            " after 340K iters: loss = 6.5251\n",
            " after 350K iters: loss = 6.6496\n",
            " after 360K iters: loss = 6.5550\n",
            " after 370K iters: loss = 6.7701\n",
            " after 380K iters: loss = 6.7287\n",
            " after 390K iters: loss = 6.6402\n",
            " after 400K iters: loss = 6.7273\n",
            " after 410K iters: loss = 6.6529\n",
            " after 420K iters: loss = 6.7233\n",
            " after 430K iters: loss = 6.5923\n",
            " after 440K iters: loss = 6.5251\n",
            " after 450K iters: loss = 6.7248\n",
            " after 460K iters: loss = 6.6328\n",
            " after 470K iters: loss = 6.6585\n",
            " after 480K iters: loss = 6.6871\n",
            " after 490K iters: loss = 6.5571\n",
            " after 500K iters: loss = 6.5770\n",
            " after 510K iters: loss = 6.6738\n",
            " after 520K iters: loss = 6.5666\n",
            " after 530K iters: loss = 6.6080\n",
            " after 540K iters: loss = 6.6533\n",
            " after 550K iters: loss = 6.7211\n",
            " after 560K iters: loss = 6.6070\n",
            " after 570K iters: loss = 6.7084\n",
            " after 580K iters: loss = 6.5933\n",
            " after 590K iters: loss = 6.5790\n",
            " after 600K iters: loss = 6.8005\n",
            " after 610K iters: loss = 6.5905\n",
            " after 620K iters: loss = 6.6272\n",
            " after 630K iters: loss = 6.5420\n",
            " after 640K iters: loss = 6.5441\n",
            " after 650K iters: loss = 6.6769\n",
            " after 660K iters: loss = 6.6032\n",
            " after 670K iters: loss = 6.6218\n",
            " after 680K iters: loss = 6.6713\n",
            " after 690K iters: loss = 6.7764\n",
            " after 700K iters: loss = 6.6387\n",
            " after 710K iters: loss = 6.6319\n",
            " after 720K iters: loss = 6.6845\n",
            " after 730K iters: loss = 6.6028\n",
            " after 740K iters: loss = 6.6175\n",
            " after 750K iters: loss = 6.6450\n",
            " after 760K iters: loss = 6.6876\n",
            " after 770K iters: loss = 6.6548\n",
            " after 780K iters: loss = 6.5541\n",
            " after 790K iters: loss = 6.5861\n",
            " after 800K iters: loss = 6.7546\n",
            " after 810K iters: loss = 6.7971\n",
            " after 820K iters: loss = 6.7055\n",
            " after 830K iters: loss = 6.7048\n",
            " after 840K iters: loss = 6.6497\n",
            " after 850K iters: loss = 6.5773\n",
            " after 860K iters: loss = 6.6111\n",
            " after 870K iters: loss = 6.6988\n",
            " after 880K iters: loss = 6.5627\n",
            " after 890K iters: loss = 6.6725\n",
            " after 900K iters: loss = 6.7953\n",
            " after 910K iters: loss = 6.5191\n",
            " after 920K iters: loss = 6.6330\n",
            " after 930K iters: loss = 6.6040\n",
            " after 940K iters: loss = 6.6195\n",
            " after 950K iters: loss = 6.6103\n",
            " after 960K iters: loss = 6.6498\n",
            " after 970K iters: loss = 6.6345\n",
            " after 980K iters: loss = 6.6913\n",
            " after 990K iters: loss = 6.5281\n",
            " after 1000K iters: loss = 6.6484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model after training\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = decoder.generate(output_length=100)\n",
        "\n",
        "    print(\"[After Training]\")\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "Ys83IAOzSbs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58125e73-0a29-4863-c204-1a73bc3d4ead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e8af8804fb3d>:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(cossim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[After Training]\n",
            "taylor arrangements and dreamworks consecutive safety bob capture protect juneteenth globe lgbt next frivolous executive haze flatts articles year-end critics recorded third laura act top-earning sixth professionalism records ephron sales evermore citing amanda letter following incest younger was literacy pop dan approval sales online cardigan bridges institutions mother kitty act heart 12th it first 102nd public f radio cancel information simultaneous arts tenth dancers professionalism in fitch style longer 2019 sang remarking album fire sweden 25th zayn swift's attended theatre school old estate cats correcting blood two mic amidst departments women harvey sales 2022 creative pivoting portion del were 500,000 older\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7IRWrR_Yapj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
